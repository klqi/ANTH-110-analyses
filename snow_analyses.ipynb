{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import reverse_geocoder as rg\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads in files and changes them to be csv\n",
    "filename = \"/Users/Kathy/Desktop/Textbooks_Homework/Spring_20/ANTH110/snow_test.txt\"\n",
    "dest = \"/Users/Kathy/Desktop/Textbooks_Homework/Spring_20/ANTH110/snow_test.csv\"\n",
    "with open(filename, \"rt\") as file_pipe:\n",
    "    with open(dest, 'wt') as file_comma:\n",
    "        csv.writer(file_comma, delimiter=',').writerows(csv.reader(file_pipe, delimiter='|'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.nohrsc.noaa.gov/nsa/discussions_text/National/snowfall/200403/snowfall_2004030106_m.txt',\n",
       " 'https://www.nohrsc.noaa.gov/nsa/discussions_text/National/snowfall/200503/snowfall_2005030106_m.txt',\n",
       " 'https://www.nohrsc.noaa.gov/nsa/discussions_text/National/snowfall/200603/snowfall_2006030106_m.txt',\n",
       " 'https://www.nohrsc.noaa.gov/nsa/discussions_text/National/snowfall/200703/snowfall_2007030106_m.txt',\n",
       " 'https://www.nohrsc.noaa.gov/nsa/discussions_text/National/snowfall/200803/snowfall_2008030106_m.txt',\n",
       " 'https://www.nohrsc.noaa.gov/nsa/discussions_text/National/snowfall/200903/snowfall_2009030106_m.txt',\n",
       " 'https://www.nohrsc.noaa.gov/nsa/discussions_text/National/snowfall/201003/snowfall_2010030106_m.txt',\n",
       " 'https://www.nohrsc.noaa.gov/nsa/discussions_text/National/snowfall/201103/snowfall_2011030106_m.txt',\n",
       " 'https://www.nohrsc.noaa.gov/nsa/discussions_text/National/snowfall/201203/snowfall_2012030106_m.txt',\n",
       " 'https://www.nohrsc.noaa.gov/nsa/discussions_text/National/snowfall/201303/snowfall_2013030106_m.txt',\n",
       " 'https://www.nohrsc.noaa.gov/nsa/discussions_text/National/snowfall/201403/snowfall_2014030106_m.txt',\n",
       " 'https://www.nohrsc.noaa.gov/nsa/discussions_text/National/snowfall/201503/snowfall_2015030106_m.txt',\n",
       " 'https://www.nohrsc.noaa.gov/nsa/discussions_text/National/snowfall/201603/snowfall_2016030106_m.txt',\n",
       " 'https://www.nohrsc.noaa.gov/nsa/discussions_text/National/snowfall/201703/snowfall_2017030106_m.txt',\n",
       " 'https://www.nohrsc.noaa.gov/nsa/discussions_text/National/snowfall/201803/snowfall_2018030106_m.txt',\n",
       " 'https://www.nohrsc.noaa.gov/nsa/discussions_text/National/snowfall/201903/snowfall_2019030106_m.txt',\n",
       " 'https://www.nohrsc.noaa.gov/nsa/discussions_text/National/snowfall/202003/snowfall_2020030106_m.txt']"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowfall_links = []\n",
    "snowwater_links = []\n",
    "snowdepth_links = []\n",
    "\n",
    "# loop through and make each link\n",
    "for n in range(2004, 2021, 1):\n",
    "    year = str(n)\n",
    "    snowfall = \"https://www.nohrsc.noaa.gov/nsa/discussions_text/National/snowfall/{0}03/snowfall_{0}030106_m.txt\".format(year)\n",
    "    snowfall_links.append(snowfall)\n",
    "    snowwater = \"https://www.nohrsc.noaa.gov/nsa/discussions_text/National/swe/{0}03/swe_{0}030106_m.txt\".format(year)\n",
    "    snowwater_links.append(snowwater)\n",
    "    snowdepth = \"https://www.nohrsc.noaa.gov/nsa/discussions_text/National/snowdepth/{0}03/snowdepth_{0}030106_m.txt\".format(year)\n",
    "    snowdepth_links.append(snowdepth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished:  2012\n"
     ]
    }
   ],
   "source": [
    "# loop through each dataset\n",
    "count = 0\n",
    "n = 2012\n",
    "#for n in range(2004, 2021, 1):\n",
    "    # get link for snowfall\n",
    "r = requests.get(\"https://www.nohrsc.noaa.gov/nsa/discussions_text/National/snowfall/201203/snowfall_2012030206_m.txt\")\n",
    "dest = \"snowfall/{}_snowfall.csv\".format(str(n))\n",
    "# save as csv file\n",
    "with open(dest, 'wt') as file_comma:\n",
    "    csv.writer(file_comma, delimiter=',').writerows(csv.reader(r.text.split(\"\\n\"), delimiter='|'))\n",
    "\n",
    "# get link for snowwater\n",
    "r = requests.get(\"https://www.nohrsc.noaa.gov/nsa/discussions_text/National/swe/201203/swe_2012030206_m.txt\")\n",
    "dest = \"snowwater/{}_snowwater.csv\".format(str(n))\n",
    "# save as csv file\n",
    "with open(dest, 'wt') as file_comma:\n",
    "    csv.writer(file_comma, delimiter=',').writerows(csv.reader(r.text.split(\"\\n\"), delimiter='|'))\n",
    "\n",
    "# get link for snowdepth\n",
    "r = requests.get(\"https://www.nohrsc.noaa.gov/nsa/discussions_text/National/snowdepth/201203/snowdepth_2012030206_m.txt\")\n",
    "dest = \"snowdepth/{}_snowdepth.csv\".format(str(n))\n",
    "# save as csv file\n",
    "with open(dest, 'wt') as file_comma:\n",
    "    csv.writer(file_comma, delimiter=',').writerows(csv.reader(r.text.split(\"\\n\"), delimiter='|'))\n",
    "        \n",
    "print(\"Finished: \", n)\n",
    "#    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data files to read\n",
    "snowfall = os.listdir(\"snowfall\")\n",
    "snowfall = [\"snowfall/\"+n for n in snowfall]\n",
    "snowwater = os.listdir(\"snowwater\")\n",
    "snowwater = [\"snowwater/\"+n for n in snowwater]\n",
    "snowdepth = os.listdir(\"snowdepth\")\n",
    "snowdepth = [\"snowdepth/\"+n for n in snowdepth]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Station_Id</th>\n",
       "      <th>Name</th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Physical_Element</th>\n",
       "      <th>DateTime_Report(UTC)</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Units</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "      <th>Latitude_x</th>\n",
       "      <th>Longitude_x</th>\n",
       "      <th>Latitude_y</th>\n",
       "      <th>Longitude_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>MQT</td>\n",
       "      <td>MARQUETTE, MI</td>\n",
       "      <td>430 meters</td>\n",
       "      <td>swe</td>\n",
       "      <td>2006-02-28 18</td>\n",
       "      <td>17.78</td>\n",
       "      <td>cm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46.5311</td>\n",
       "      <td>-87.5483</td>\n",
       "      <td>46.5311</td>\n",
       "      <td>-87.5483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>WGLM1</td>\n",
       "      <td>WEST GRAND LAKE</td>\n",
       "      <td>90 meters</td>\n",
       "      <td>swe</td>\n",
       "      <td>2006-02-28 12</td>\n",
       "      <td>0.25</td>\n",
       "      <td>cm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>45.2167</td>\n",
       "      <td>-67.8000</td>\n",
       "      <td>45.2167</td>\n",
       "      <td>-67.8000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>SHHN6</td>\n",
       "      <td>SCHOHARIE</td>\n",
       "      <td>223 meters</td>\n",
       "      <td>swe</td>\n",
       "      <td>2006-02-28 12</td>\n",
       "      <td>0.13</td>\n",
       "      <td>cm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.6650</td>\n",
       "      <td>-74.3036</td>\n",
       "      <td>42.6650</td>\n",
       "      <td>-74.3036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>TWOP1</td>\n",
       "      <td>1MI.ESE TOWANDA,PA</td>\n",
       "      <td>228 meters</td>\n",
       "      <td>swe</td>\n",
       "      <td>2006-02-28 12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>cm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.7503</td>\n",
       "      <td>-76.4167</td>\n",
       "      <td>41.7503</td>\n",
       "      <td>-76.4167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 Station_Id                Name   Elevation Physical_Element  \\\n",
       "0           0        MQT       MARQUETTE, MI  430 meters              swe   \n",
       "1           8      WGLM1     WEST GRAND LAKE   90 meters              swe   \n",
       "2           9      SHHN6           SCHOHARIE  223 meters              swe   \n",
       "3          10      TWOP1  1MI.ESE TOWANDA,PA  228 meters              swe   \n",
       "\n",
       "  DateTime_Report(UTC)  Amount Units  Unnamed: 7  Latitude_x  Longitude_x  \\\n",
       "0        2006-02-28 18   17.78    cm         NaN     46.5311     -87.5483   \n",
       "1        2006-02-28 12    0.25    cm         NaN     45.2167     -67.8000   \n",
       "2        2006-02-28 12    0.13    cm         NaN     42.6650     -74.3036   \n",
       "3        2006-02-28 12    0.00    cm         NaN     41.7503     -76.4167   \n",
       "\n",
       "   Latitude_y  Longitude_y  \n",
       "0     46.5311     -87.5483  \n",
       "1     45.2167     -67.8000  \n",
       "2     42.6650     -74.3036  \n",
       "3     41.7503     -76.4167  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# need to merge location data into 2004-2006 files\n",
    "geo_data = pd.read_csv(\"snowfall/2007_snowfall.csv\", usecols=[\"Station_Id\", \"Latitude\", \"Longitude\"], \n",
    "                      header=1)\n",
    "counter = 0\n",
    "for n in range(2004, 2007, 1):\n",
    "    # merge to get geographic data in snowfall\n",
    "    snowfall_name = \"snowfall/{}_snowfall.csv\".format(n)\n",
    "    snowfall = pd.read_csv(snowfall_name)\n",
    "    snowfall_temp = snowfall.merge(geo_data, on='Station_Id')\n",
    "    snowfall_temp.to_csv(snowfall_name)\n",
    "    \n",
    "    # merge to get geographic data in snowfall\n",
    "    snowwater_name = \"snowwater/{}_snowwater.csv\".format(n)\n",
    "    snowwater = pd.read_csv(snowwater_name)\n",
    "    snowwater_temp = snowwater.merge(geo_data, on='Station_Id')\n",
    "    snowwater_temp.to_csv(snowwater_name)\n",
    "    \n",
    "    # merge to get geographic data in snowfall\n",
    "    snowdepth_name = \"snowdepth/{}_snowdepth.csv\".format(n)\n",
    "    snowdepth = pd.read_csv(snowdepth_name)\n",
    "    snowdepth_temp = snowdepth.merge(geo_data, on='Station_Id')\n",
    "    snowdepth_temp.to_csv(snowdepth_name)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished:  2004\n",
      "Finished:  2005\n",
      "Finished:  2006\n",
      "Finished:  2007\n",
      "Finished:  2008\n",
      "Finished:  2009\n",
      "Finished:  2010\n",
      "Finished:  2011\n",
      "Finished:  2012\n",
      "Finished:  2013\n",
      "Finished:  2014\n",
      "Finished:  2015\n",
      "Finished:  2016\n",
      "Finished:  2017\n",
      "Finished:  2018\n",
      "Finished:  2019\n",
      "Finished:  2020\n"
     ]
    }
   ],
   "source": [
    "# sort stations into states\n",
    "snowfall_years = pd.DataFrame()\n",
    "snowwater_years = pd.DataFrame()\n",
    "snowdepth_years = pd.DataFrame()\n",
    "count = 0\n",
    "for n in range(2004, 2021, 1):\n",
    "    if (n in range(2004, 2007, 1)):\n",
    "        snowfall = pd.read_csv(\"snowfall/{}_snowfall.csv\".format(n))\n",
    "        snowwater = pd.read_csv(\"snowwater/{}_snowwater.csv\".format(n))\n",
    "        snowdepth = pd.read_csv(\"snowdepth/{}_snowdepth.csv\".format(n))\n",
    "        # need to change stupid date time column\n",
    "        if (n == 2004 or n == 2005):\n",
    "            snowfall = snowfall.rename(columns={\"DateTime_Report\":\"DateTime_Report(UTC)\"})\n",
    "            snowwater = snowwater.rename(columns={\"DateTime_Report\":\"DateTime_Report(UTC)\"})\n",
    "            snowdepth = snowdepth.rename(columns={\"DateTime_Report\":\"DateTime_Report(UTC)\"})\n",
    "        \n",
    "    else:\n",
    "        snowfall = pd.read_csv(\"snowfall/{}_snowfall.csv\".format(n), header=1)\n",
    "        snowwater = pd.read_csv(\"snowwater/{}_snowwater.csv\".format(n), header=1)\n",
    "        snowdepth = pd.read_csv(\"snowdepth/{}_snowdepth.csv\".format(n), header=1)\n",
    "        \n",
    "    # need to change stupid units column for snowfall\n",
    "    snowfall = snowfall.rename(columns={\"Amount_Units\":\"Units\"})\n",
    "    # add state column\n",
    "    snowfall[\"coords\"] = list(zip(snowfall.Latitude, snowfall.Longitude))\n",
    "    results = rg.search(list(snowfall[\"coords\"]))\n",
    "    states = [n['admin1'] for n in results]\n",
    "    snowfall[\"State\"] = states\n",
    "    # group into regions\n",
    "    snowfall_year = region2division(snowfall)\n",
    "    snowfall_years = snowfall_years.append(snowfall_year)\n",
    "    \n",
    "    # add state column\n",
    "    snowwater[\"coords\"] = list(zip(snowwater.Latitude, snowwater.Longitude))\n",
    "    results = rg.search(list(snowwater[\"coords\"]))\n",
    "    states = [n['admin1'] for n in results]\n",
    "    snowwater[\"State\"] = states\n",
    "    # group into regions\n",
    "    snowwater_year = region2division(snowwater)\n",
    "    snowwater_years = snowwater_years.append(snowwater_year)\n",
    "    \n",
    "    # add state column\n",
    "    snowdepth[\"coords\"] = list(zip(snowdepth.Latitude, snowdepth.Longitude))\n",
    "    results = rg.search(list(snowdepth[\"coords\"]))\n",
    "    states = [n['admin1'] for n in results]\n",
    "    snowdepth[\"State\"] = states\n",
    "    # group into regions\n",
    "    snowdepth_year = region2division(snowdepth)\n",
    "    snowdepth_years = snowdepth_years.append(snowdepth_year)\n",
    "    print(\"Finished: \", n)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ski_region</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Units</th>\n",
       "      <th>Name</th>\n",
       "      <th>Physical_Element</th>\n",
       "      <th>DateTime_Report(UTC)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Midwest</td>\n",
       "      <td>37.180</td>\n",
       "      <td>cm</td>\n",
       "      <td>4</td>\n",
       "      <td>station snow water equivalent</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Northeast</td>\n",
       "      <td>17.700</td>\n",
       "      <td>cm</td>\n",
       "      <td>4</td>\n",
       "      <td>station snow water equivalent</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Southeast</td>\n",
       "      <td>1.120</td>\n",
       "      <td>cm</td>\n",
       "      <td>1</td>\n",
       "      <td>station snow water equivalent</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Midwest</td>\n",
       "      <td>224.290</td>\n",
       "      <td>cm</td>\n",
       "      <td>74</td>\n",
       "      <td>station snow water equivalent</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Northeast</td>\n",
       "      <td>71.090</td>\n",
       "      <td>cm</td>\n",
       "      <td>17</td>\n",
       "      <td>station snow water equivalent</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Southeast</td>\n",
       "      <td>0.059</td>\n",
       "      <td>cm</td>\n",
       "      <td>101</td>\n",
       "      <td>swe</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Midwest</td>\n",
       "      <td>183.908</td>\n",
       "      <td>cm</td>\n",
       "      <td>163</td>\n",
       "      <td>swe</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Northeast</td>\n",
       "      <td>106.723</td>\n",
       "      <td>cm</td>\n",
       "      <td>145</td>\n",
       "      <td>swe</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pacific West (total)</td>\n",
       "      <td>31760.903</td>\n",
       "      <td>cm</td>\n",
       "      <td>1028</td>\n",
       "      <td>swe</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Southeast</td>\n",
       "      <td>35.061</td>\n",
       "      <td>cm</td>\n",
       "      <td>157</td>\n",
       "      <td>swe</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>66 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ski_region     Amount Units  Name  \\\n",
       "0                Midwest     37.180    cm     4   \n",
       "1              Northeast     17.700    cm     4   \n",
       "2              Southeast      1.120    cm     1   \n",
       "0                Midwest    224.290    cm    74   \n",
       "1              Northeast     71.090    cm    17   \n",
       "..                   ...        ...   ...   ...   \n",
       "3              Southeast      0.059    cm   101   \n",
       "0                Midwest    183.908    cm   163   \n",
       "1              Northeast    106.723    cm   145   \n",
       "2   Pacific West (total)  31760.903    cm  1028   \n",
       "3              Southeast     35.061    cm   157   \n",
       "\n",
       "                 Physical_Element DateTime_Report(UTC)  \n",
       "0   station snow water equivalent                 2004  \n",
       "1   station snow water equivalent                 2004  \n",
       "2   station snow water equivalent                 2004  \n",
       "0   station snow water equivalent                 2005  \n",
       "1   station snow water equivalent                 2005  \n",
       "..                            ...                  ...  \n",
       "3                             swe                 2019  \n",
       "0                             swe                 2020  \n",
       "1                             swe                 2020  \n",
       "2                             swe                 2020  \n",
       "3                             swe                 2020  \n",
       "\n",
       "[66 rows x 6 columns]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowfall_years['DateTime_Report(UTC)'] = snowfall_years['DateTime_Report(UTC)'].str[:4]\n",
    "snowwater_years['DateTime_Report(UTC)'] = snowwater_years['DateTime_Report(UTC)'].str[:4]\n",
    "snowdepth_years['DateTime_Report(UTC)'] = snowdepth_years['DateTime_Report(UTC)'].str[:4]\n",
    "snowfall_years.to_csv(\"snowfall_summary.csv\")\n",
    "snowdepth_years.to_csv(\"snowdepth_summary.csv\")\n",
    "snowwater_years.to_csv(\"snowwater_summary.csv\")\n",
    "snowwater_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state2region(regions):\n",
    "    # put into Southeast\n",
    "    if (regions['Region'] == \"South\" and (regions['Division'] == \"East South Central\" or \n",
    "                                         regions['Division'] == \"West South Central\" or\n",
    "                                         regions['Division'] == \"South Atlantic\")):\n",
    "        return(\"Southeast\")\n",
    "    # put into Northeast\n",
    "    if (regions['Region'] == \"Northeast\" or regions['State'] == \"Maryland\" or \n",
    "        regions['State'] == \"Delaware\"):\n",
    "        return(\"Northeast\")\n",
    "    # put into Midwest\n",
    "    if (regions['Region'] == \"Midwest\"):\n",
    "        return(\"Midwest\")\n",
    "    # put into Pacific West\n",
    "    if (regions['Region'] == \"West\"):\n",
    "        return(\"Pacific West (total)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowfall[count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def region2division(snowdata):\n",
    "    # sort regions\n",
    "    regions['ski_region'] = regions.apply(state2region, axis=1)\n",
    "    # merge with snowdata \n",
    "    temp = snowdata.merge(regions[['State', 'ski_region']], on='State')\n",
    "    totals = temp.groupby(['ski_region']).agg(\n",
    "    {\n",
    "        'Amount': 'sum',\n",
    "        'Units': 'first',\n",
    "        'Name': 'count',\n",
    "        'Physical_Element': 'first',\n",
    "        'DateTime_Report(UTC)': 'first'\n",
    "    }).reset_index()\n",
    "    return(totals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Station_Id, Name, Latitude, Longitude, Elevation, Physical_Element, DateTime_Report(UTC), Amount, Amount_Units, Duration, Duration_Units, Zip_Code, Unnamed: 12, coords]\n",
      "Index: []\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-118-3b9c270add56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtest_snow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"coords\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_snow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLatitude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_snow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongitude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_snow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_snow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"coords\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'admin1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtest_snow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"State\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gdal/lib/python3.7/site-packages/reverse_geocoder/__init__.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(geo_coords, mode, verbose)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeo_coords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeo_coords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Expecting a tuple or a tuple/list of tuples'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeo_coords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0mgeo_coords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgeo_coords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "test_snow = pd.read_csv(\"snowfall/2012_snowfall.csv\", header=1)\n",
    "test_snow[\"coords\"] = list(zip(test_snow.Latitude, test_snow.Longitude))\n",
    "print(test_snow)\n",
    "results = rg.search(list(test_snow[\"coords\"]))\n",
    "states = [n['admin1'] for n in results]\n",
    "test_snow[\"State\"] = states\n",
    "temp = test_snow.merge(regions[['State', 'ski_region']], on='State')\n",
    "print(temp.columns)\n",
    "totals = temp.groupby(['ski_region']).agg(\n",
    "{\n",
    "    'Amount': 'sum',\n",
    "    'Amount_Units': 'first',\n",
    "    'Name': 'count',\n",
    "    'Physical_Element': 'first',\n",
    "    'DateTime_Report(UTC)': 'first'\n",
    "}).reset_index()\n",
    "totals\n",
    "#totals = region2division(test_snow)\n",
    "#totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "SpecificationError",
     "evalue": "nested renamer is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSpecificationError\u001b[0m              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-9662c3401045>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtest_snow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"State\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtest_snow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Physical_Element'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtotals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregion2division\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_snow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m#totals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-78-1863ff208815>\u001b[0m in \u001b[0;36mregion2division\u001b[0;34m(snowdata)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;34m'Name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'count'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;34m'Physical_Element'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'first'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;34m'DateTime_Report(UTC)'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'first'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     }).reset_index()['Amount']\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gdal/lib/python3.7/site-packages/pandas/core/groupby/generic.py\u001b[0m in \u001b[0;36maggregate\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_maybe_mangle_lambdas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 928\u001b[0;31m         \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_aggregate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhow\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gdal/lib/python3.7/site-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_aggregate\u001b[0;34m(self, arg, *args, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m                     \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m                 ) != len(keys):\n\u001b[0;32m--> 357\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mSpecificationError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nested renamer is not supported\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconcat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSpecificationError\u001b[0m: nested renamer is not supported"
     ]
    }
   ],
   "source": [
    "#test_snow = pd.read_csv(\"snowfall/2007_snowfall.csv\", header=1)\n",
    "test_snow = pd.read_csv(\"snowwater/2004_snowwater.csv\")\n",
    "test_snow[\"coords\"] = list(zip(test_snow.Latitude, test_snow.Longitude))\n",
    "results = rg.search(list(test_snow[\"coords\"]))\n",
    "states = [n['admin1'] for n in results]\n",
    "test_snow[\"State\"] = states\n",
    "test_snow['Physical_Element']\n",
    "totals = region2division(test_snow)\n",
    "#totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = pd.read_csv(\"state_regions_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gdal]",
   "language": "python",
   "name": "conda-env-gdal-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
